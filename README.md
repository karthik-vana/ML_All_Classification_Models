# ğŸ§  MNIST Digit Classification â€“ ML Models Comparison + SVM Hyperparameter Tuning

![Python](https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=python&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-F7931E?style=flat-square&logo=scikit-learn&logoColor=white)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-FF6F00?style=flat-square&logo=tensorflow&logoColor=white)
![Status](https://img.shields.io/badge/Status-Completed-success?style=flat-square)

## ğŸ“Œ Project Overview

This project implements a **complete Handwritten Digit Classification system** using the **MNIST dataset**. I trained and compared **9 different machine learning models**, evaluated them using **ROC-AUC curves**, selected the best performer (SVM), and fine-tuned it using **GridSearchCV** for optimal performance.

---

## ğŸ¯ Project Objectives

 â–«ï¸Compare multiple ML algorithms systematically  
 â–ªï¸Visualize model performance using ROC curves  
 â–«ï¸Optimize the best model through hyperparameter tuning  
 â–«ï¸Deploy a production-ready classifier  

---

## Click here for Dataset ğŸ“šğŸ“Œ
<div align="side"> 
    
[![Kaggle](https://img.shields.io/badge/Kaggle-100000?style=for-the-badge&logo=Kaggle&logoColor=white)](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv)
</div>

## ğŸš€ Project Workflow

### 1ï¸âƒ£. Data Loading & Preprocessing
- Loaded MNIST dataset in CSV format (`label + 784 pixel features`)
- Handled missing values and data inconsistencies
- Normalized pixel values (0-255 â†’ 0-1)
- Split data into **training** and **testing** sets

### 2ï¸âƒ£. Model Training & Implementation
Built a custom Python class `MNISTModel` that trains **9 ML algorithms**:

| Model | Algorithm Type |
|-------|---------------|
| ğŸ”¹ K-Nearest Neighbors (KNN) | Instance-based |
| ğŸ”¹ Naive Bayes | Probabilistic |
| ğŸ”¹ Logistic Regression | Linear classifier |
| ğŸ”¹ Decision Tree | Tree-based |
| ğŸ”¹ Random Forest | Ensemble (Bagging) |
| ğŸ”¹ AdaBoost | Ensemble (Boosting) |
| ğŸ”¹ Gradient Boosting | Ensemble (Boosting) |
| ğŸ”¹ XGBoost | Optimized Gradient Boosting |
| ğŸ”¹ Support Vector Machine (SVM) | Kernel-based |

**Each model:**
- Prints training accuracy
- Stores trained model for comparison
- Supports probability predictions for ROC analysis

### 3ï¸âƒ£. Model Comparison Using ROC Curves
- Computed `predict_proba` for all models
- Converted **multiclass problem â†’ binary** (One-vs-Rest for Class 1)
- Plotted **all 9 ROC curves** on a single graph
- Calculated **AUC (Area Under Curve)** for each model
- **Result:** SVM with RBF kernel achieved the highest AUC â†’ selected as final model

### 4ï¸âƒ£. Hyperparameter Tuning with GridSearchCV
Fine-tuned the SVM model by testing combinations of:
```python
param_grid = {
    'kernel': ['linear', 'rbf', 'poly'],
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01],
    'decision_function_shape': ['ovr', 'ovo'],
    'probability': [True]
}
```

- Used **cross-validation**
- Identified optimal hyperparameters
- Significantly improved model stability and accuracy

### 5ï¸âƒ£. Final Model Evaluation
Trained the final SVM with best parameters and evaluated using:

ğŸ“Š **Metrics:**
- Training Accuracy
- Testing Accuracy
- **Confusion Matrix** (10Ã—10 for digits 0-9)
- **Classification Report** (Precision, Recall, F1-Score per digit)

### 6ï¸âƒ£. Model Deployment
- Saved the final trained SVM using `pickle`
- Tested predictions on random 784-pixel inputs
- Model ready for production deployment

---

## ğŸ” Key Insights & Learnings

ğŸ’¡ **ROC-AUC provides better model comparison** than simple accuracy, especially for imbalanced datasets  
ğŸ’¡ **SVM with RBF kernel consistently outperformed** other traditional ML models on MNIST  
ğŸ’¡ **GridSearchCV tuning significantly improved** model generalization and reduced overfitting  
ğŸ’¡ **Object-Oriented Programming (OOP)** approach with classes made code modular and maintainable  
ğŸ’¡ **Exception handling** ensured robust, production-ready code  

---

## ğŸ“‚ Project Structure
```
ğŸ“¦ MNIST-Digit-Classification
â”œâ”€â”€ ğŸ“„ mnist_classification.ipynb    # Main Jupyter Notebook
â”œâ”€â”€ ğŸ“„ mnist_model.py                # MNISTModel class implementation
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ mnist_train.csv
â”‚   â””â”€â”€ mnist_test.csv
â”œâ”€â”€ ğŸ“ models/
â”‚   â””â”€â”€ best_svm_model.pkl          # Saved final model
â”œâ”€â”€ ğŸ“ visualizations/
â”‚   â”œâ”€â”€ roc_curves.png
â”‚   â””â”€â”€ confusion_matrix.png
â””â”€â”€ ğŸ“„ README.md
```

---

## ğŸ› ï¸ Technologies & Libraries Used

**Languages:**
- Python 3.x

**Libraries:**
```python
# Data Manipulation
import numpy as np
import pandas as pd

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, auc, roc_auc_score

# ML Algorithms
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Model Persistence
import pickle
```

---

## ğŸ“Š Results Summary

| Model | Test Accuracy | ROC-AUC (Class 1) |
|-------|--------------|-------------------|
| KNN | ~96.5% | 0.985 |
| Naive Bayes | ~84.2% | 0.921 |
| Logistic Regression | ~92.3% | 0.968 |
| Decision Tree | ~87.6% | 0.894 |
| Random Forest | ~96.8% | 0.992 |
| AdaBoost | ~79.4% | 0.912 |
| Gradient Boosting | ~95.7% | 0.989 |
| XGBoost | ~96.9% | 0.993 |
| **SVM (RBF)** | **97.8%** | **0.997** âœ… |
| **SVM (Tuned)** | **98.2%** | **0.998** ğŸ† |

*Note: Replace with your actual results*

---

## ğŸš€ How to Run This Project

### Prerequisites
```bash
pip install numpy pandas scikit-learn xgboost matplotlib seaborn
```

### Steps
1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/MNIST-Digit-Classification.git
cd MNIST-Digit-Classification
```

2. **Run the Jupyter Notebook:**
```bash
jupyter notebook mnist_classification.ipynb
```

3. **Or run the Python script:**
```bash
python mnist_model.py
```

4. **Load the saved model for predictions:**
```python
import pickle

# Load model
with open('models/best_svm_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Make prediction
prediction = model.predict([your_784_pixel_array])
print(f"Predicted Digit: {prediction[0]}")
```

---

## ğŸ“ˆ Sample Visualizations

### ROC Curves Comparison
![ROC Curves](Roc-Auc_curve.jpg)
*All 9 models plotted together showing SVM's superior performance*

---

## ğŸ“ Learning Outcomes

Through this project, I gained hands-on experience with:

âœ… **Multi-model ML pipeline development**  
âœ… **ROC curve analysis and interpretation**  
âœ… **Hyperparameter optimization techniques**  
âœ… **Object-oriented ML code design**  
âœ… **Model evaluation best practices**  
âœ… **Production-ready model deployment**  

---

## ğŸ”® Future Enhancements

- [ ] Implement **Deep Learning** models (CNN) for comparison
- [ ] Create a **web app** using Streamlit/Flask for digit drawing and prediction
- [ ] Add **model explainability** using LIME/SHAP
- [ ] Optimize for **real-time inference** speed
- [ ] Deploy on **cloud platform** (AWS/GCP/Azure)

---

## ğŸ™ Acknowledgements

First & most a very Special thanks to [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/sai-kamal-korlakunta/), Trainer & CEO at **Vihara Tech** ğŸ‘‰ [viharatech.com](https://www.viharatech.com/)  
For his invaluable guidance throughout my **Data Science with Gen AI** journey.

---

## ğŸŒ Connect With Me

<div align="center">

[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/karthik-vana)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/karthik-vana)
[![HackerRank](https://img.shields.io/badge/HackerRank-2EC866?style=for-the-badge&logo=hackerrank&logoColor=white)](https://www.hackerrank.com/profile/karthikvana236)
[![Portfolio](https://img.shields.io/badge/Portfolio-FF5722?style=for-the-badge&logo=google-chrome&logoColor=white)](https://your-portfolio-link.com)

</div>

---

## ğŸ“œ License

This project is open-source and available under the [MIT License](LICENSE).

---

<div align="center">

**â­ If you find this project helpful, please consider giving it a star! â­**

*Built with â¤ï¸ & pythonğŸ as part of my Data Science journey*

</div>
